{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Sentiment Analysis With Single Layer Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We start by importing our dependencies. We will use nltk for an annotated dataset with movie reviews - categorized as negative or positive. Our goal is a classifier that given an unseen movie review categorizes that review correctly. As this is a linearly separable problem, we can use the perceptron algorithm (a singel neuron and the building block of neural nets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from random import shuffle\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have to do some data preparation. We construct a tuple for every movie review containing a list with all the words in the review and the category (neg or pos). Then we shuffle the training data to make sure the ordering is random. We find the frequency distribution of all words in the movie reviews, and pick out the 2000 most common words as our feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for category in movie_reviews.categories():\n",
    "    for file_id in movie_reviews.fileids(category):\n",
    "        data_tuple = (list(movie_reviews.words(file_id)), category)\n",
    "        training_data.append(data_tuple)\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "word_freqs = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "\n",
    "features = [w for (w,_) in word_freqs.most_common(2000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we need to make numerical representation of the words for our neuron to be able to make sense of it. We choose to make a list of integers: 1 represents a positive review, and negative 1 represents a negative review. We split up the training data so that only 3/4 of the data is used to train the neuron. 1/4 will be used to test the accuracy of our classifier. Our list of integers consists of one integer per word in the 2000 most frequent words in our corpus - again we will use 1 and negative 1 as representations. We append 1 to the list every time we find a word in our training data that are also in the most frequent words list. -1 for every word in the most frequent words list that does not exist in the current data. These integer representations follows the same order for the 2000 words for all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_data_int_rep = []\n",
    "\n",
    "for i in range(0, 1500):\n",
    "    data = training_data[i]\n",
    "    int_list = []     \n",
    "    training_data_int_rep.append((int_list, data[1]))\n",
    "    for word_f in features:     \n",
    "        if word_f in data[0]:    \n",
    "            int_list.append(1) \n",
    "        else: \n",
    "            int_list.append(-1) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's time to define our Neuron class with a simple perceptron algorithm. We initialize our neuron with random weights for every connection from an input node to our neuron. Our learning constant will affect the rate of change for the gradient descent based error correction. The neurons mission is to map the pattern of our input vectors to the expected value in the binary output range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, number_of_weigths):\n",
    "       \n",
    "        self.input_weights = []\n",
    "        for i in range(0, number_of_weigths):\n",
    "            self.input_weights.append(random.uniform(-1,1))\n",
    "        self.learning_constant = 0.01\n",
    "  \n",
    "    \n",
    "    def learn(self, inputs, correct_output):\n",
    "  \n",
    "        prediction = self.prediction(inputs)\n",
    "        deviation = correct_output - prediction\n",
    "        \n",
    "        for i in range(0, len(self.input_weights)):\n",
    "            self.input_weights[i] += self.learning_constant * deviation * inputs[i]\n",
    "    \n",
    "  \n",
    "    def prediction(self, inputs):\n",
    "    \n",
    "        sum = 0;  \n",
    "        for i in range(0, len(self.input_weights)):\n",
    "            sum += inputs[i] * self.input_weights[i]\n",
    "           \n",
    "        if sum > 0: return 1 \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2000 inputs will enter our neuron with every review. For the training effort we also send in the expected output as an argument. We use 4 epochs (complete forward and backward pass of the training set) to bump up our accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "neuron = Neuron(2000)\n",
    "\n",
    "epochs = 4\n",
    "for i in range(0,epochs):\n",
    "    for data in training_data_int_rep:\n",
    "        if data[1] == \"pos\":\n",
    "            correct_output = 1\n",
    "        else: \n",
    "            correct_output = -1\n",
    "        neuron.learn(data[0], correct_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can now test our classifier on our test set. 500 reviews that was not used in the training process will be used to test prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_data_int_rep = []\n",
    "\n",
    "for i in range(1500, len(training_data)):\n",
    "    data = training_data[i]\n",
    "    int_list = []     \n",
    "    test_data_int_rep.append((int_list, data[1]))\n",
    "    \n",
    "    for word_f in features:     \n",
    "        if word_f in data[0]:    \n",
    "            int_list.append(1) \n",
    "        else: \n",
    "            int_list.append(-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.798\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "for data in test_data_int_rep:\n",
    "    if data[1] == \"pos\":\n",
    "        correct_output = 1\n",
    "    else: \n",
    "        correct_output = -1\n",
    "    \n",
    "    if neuron.prediction(data[0]) == correct_output:\n",
    "        correct_predictions+=1\n",
    "\n",
    "length_of_test_set = 500\n",
    "print(\"Accuracy: \", correct_predictions/length_of_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "About 0.8 accuracy rate for our quite limited training data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
